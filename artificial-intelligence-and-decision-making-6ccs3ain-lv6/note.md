## 介绍

人工智能主要从两个方面描述：“复刻**人类行为**”和“**理性**”。而大家对”智能“的主题也有两种看法：”智能“是**内部思维推理**，还是**外在行为**的表现。这两个维度会诞生四种不同的人工智能发展方向。

1. 类人行为：比如典型的图灵测试就是测试这类人工智能的，为了“骗过人类”。这类人工智能需要具备以下能力：

   - 自然语言处理：能听和说人类的语言。
   - 知识表示：能存储它接受的知识。
   - 自动推理：回答问题得出结论。
   - 机器学习：适应新环境，推断模式。

   以及可选的计算机视觉和机器人学（不同的输入输出方式吧）。

2. 类人思考：认知科学致力于研究人工智能的计算机模型和心理学。我们可以通过内省（反思自己思考的过程），心理实验（观察受试者的行为）和大脑成像等方式了解人类思维，并让程序模拟这些理论。

3. 理性思考：主要是通过逻辑推理（根据已知的内容推出结论，如：苏格拉底是人，人都是凡人，则苏格拉底是凡人）和概率（弥补不确定的条件），建立理性思维模型。

4. 理性行为：为取得最佳的（确定的或者期望的）结果而采取行动。

理性行为看起来优于类人行为，因为人类的思维模式可能会有很多无法达到的最优解（也就是：**可计算性**和**易处理性**的平衡）。但是完美理性计算代价太高了，环境因素过于复杂。所以后面会讲到有限理性，适当地采取一些行动。

AI：可以进行“思考”，做出决策，给其他软硬件分配任务。

> 做决策并不是 if else 那么简单，并不能涵盖大多数情况。23年我参加ST峰会的时候第一次开始了解AI，当时工作人员介绍他们的工厂电机可以通过深度学习判断一些异常状况（比如设备未水平，或者有异物卡住电机等）来停止电机。我当时就问，我说这为啥要用深度学习呢，不用不也能解决，他和我说状态机并不总是能准确判断所有情况的，利用深度学习总结一些已有模式，决策才会更加准确快速。23年ST的峰会主题之一就是AI的边缘侧应用。

Simple Agent： 给每种 percept 认知指向一个 action。比如，0是周一，1是周二，2是周三……检测到地面脏就开启吸尘，否则拖地……

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202409302259773.png" alt="image-20240930225951674" style="zoom:50%;" />

弊端非常明显，首先代码量很大，情况太多的话速度也会变慢，比如GTA5里有19.8亿次的if。而且我们并不能准确判断出所有情况（地面怎样算脏？90%的部分被遮挡就算脏吗？边缘的90%和中部的90%情况如何清扫？）

所以对于所有可能的情况都准确已知的情况下，simple agent才更好。但是我们的环境中变量，影响因素太多了，大多数时候没有这么简单的情况。

Utility-based agents：穷举出当前情况的所有actions，以及其效用，并递归推导下一步。比如一枚象棋，可以先往哪些方向走，到达这些方向后又可以继续往哪些方向走……我们会发现有的方向走不了因为被其他棋子挡住，有的方向不能走因为可能走到对方棋子攻击范围内了，这些步骤的效用都不好都可以舍弃。

我们还可以根据这些数据计算出期望效用值。

但是效用代理的缺点也很明显，一方面消耗大量资源时间（穷举），另一方面很难看得长远，比如国际象棋棋局，这种代理方式很可能只按照每一步的最佳走法走，长远目光不行。

## LLM

通过构建单词短语之间的关联而建立。根据上文猜测下文应该说的内容，GPT就是非常典型的例子。

缺点在于：

1. 数据全部来源于训练日期前的数据。
2. 训练底层不透明（可能意思是我们很难知道模型被训练成什么样了？）
3. 可能会幻想，自己编造数据。
4. 如果数据来源不准确，不全面，有偏见等，也会影响生成的结果。所以需要很多评估，比如是否有偏见，数据准确度，语调等。

## 概率

这一段基本和机器学习是一样的内容，概率，条件概率，独立，贝叶斯公式等。

马尔科夫链：每个节点都条件独立于其他结点。**一个结点的马尔科夫链是他的父节点，他的子节点，和他的子节点的其他父节点**。

我们可以先用变量描述要解决的问题，然后将其转化为相互条件概率链接的马尔科夫链解决问题。

### Inference by enumeration 枚举推理法

列出所有条件概率。很明显这种方法能涵盖所有情况但是效率低。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202410091446081.png" alt="image-20241009144650015" style="zoom:67%;" />

### Prior sampling 先验抽样

> 1）先验——根据若干年的统计（经验）或者气候（常识），某地方下雨的概率；
>
> 2）似然——下雨（果）的时候有乌云（因/证据/观察的数据）的概率，即已经有了果，对证据发生的可能性描述；
>
> 3）后验——根据天上有乌云（原因或者证据/观察数据），下雨（结果）的概率；
>
> 后验 ~ 先验*似然 ： 存在下雨的可能（先验），下雨之前会有乌云（似然）~ 通过现在有乌云推断下雨概率（后验）
>
> [机器学习中的先验、后验和似然_先验后验背景-CSDN博客](https://blog.csdn.net/qq_39905917/article/details/83035386)

我们随机抽样一些数字，通过其最终概率规律来预测最终结果，而不是精准地计算出所有条件概率。

因为现实世界中变量概率影响因素太多，这样计算负担过大。

先验采样适用于联合概率。

比如下面这个题：

![image-20241009154642824](https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202410091546895.png)

我们随机生成一个概率序列：0.4 0.2 0.71 0.2

0.4<P(C)，所以C发生。

0.2>=P(S|C)，所以S没发生。

0.71<P(R|C)，所以R发生。

S没发生，R发生的情况下，P(W|S,R)=0.9>0.2，所以W发生。

那么这次采样结果就是：C, not S, R, W。

多采样几次总结出序列 P(c, not s, r, w) 的发生概率，采样越多越精确。

但是先验概率没法算条件概率。

### Rejection Sampling 接受-拒绝定理

比如计算P(X|e)，我们先用先验概率采样所有点，只选择其中落在e中的点进行统计。对于每个点i，让N[X=i]++统计总数，最后正交化数组N[X]统计所有在e范围内的X的概率分布情况。

简单来说就是落在e之外的不用嘛。不过会带来的问题在于，如果e特别小，那么就会浪费很多采样点没用。

### Likelihood weighting

已经观测到结果，条件发生的可能性。

采用最大最有可能发生的概率。

如题：

![image-20241009165638431](https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202410091656513.png)

也就是说，现在观测到下雨 Rain了，那么Cloudy和WetGrass=true的发生概率有多大？

还是按顺序推导。首先设权重w=1.

想满足C=true：让w=0.5

S没要求（not a evidence variable）所以=多少都行，不用改权重值。假设S=false。

R也是同理，不是条件，假设R=true.

想让W=true，w=0.5*0.9=0.45.

所以权重为0.95的时候最可能发生有云且草地湿了后下雨了。

### Gibbs sampling

有点抽象，从一道例题来看吧。

条件概率图如下：

![image-20241009173512060](https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202410091735113.png)

我们想计算： P(Cloudy|Sprinker = true, WetGrass = true)

1. 随机初始化所有变量。evidence variables 的值要固定为题目要求，也就是 Sprinker = true, WetGrass = true。假设初始化得到的是 [Cloudy = true, Rain = false, Spinker = true, WetGrass = true]
2. 先采样 C，其他变量不变。获取 C 的马尔科夫链（S, R），从 P[C | S=true, R=false] 和 P[C=false | S=true, R=false] 中取样。采样完了根据概率分布决定C的值，假设是false。
3. 然后采样 R，其马尔科夫链是（C, S, WG），采样 P[R | C=false, S=true, W=true] 
4. 可能重复上述步骤多次。最后根据采样结果估算这些采样中 P(Cloudy|Sprinker = true, WetGrass = true) 的概率。

## Sequential decision making 顺序决策

比如走迷宫，每个结点依赖于之前的结点的选择。

decision making 的要点在于简化决策。主要有两种方法做决策：

1. 永远找最好的结果。
2. 永远规避最坏的结果。

### 乐观版：尽可能选择最好的结果

例题：投硬币，正面赔2元，反面赚3元，玩合适吗？

算数学期望，平均能得到0.5元，不玩0元。合适。

连玩10次的期望值就是5元。

*想起了齐先生的赌场老虎机*

当然并不是说玩了就能得到5毛钱，这只是“数学期望”，在数据量上来的情况下更可能偏向的结果。

制定决策其实就是转换为概率问题，我们在每个节点处选择概率可能性更大的结果，比如吃豆人往前走得分的期望是0.5，往左走是-1，往右走是-2，那么我们最终结果最好偏向于往前走。

吃豆人结束一整局后的分数期望就是利用 sequence 中每一步选择的期望效用（utility）计算最终效用。

### 悲观版：避开最坏的结果

比如下面这个图，乐观版追求最佳结果s6，选择a2方向。悲观版为了避开s4，选择a1方向。

![image-20241016232906205](https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202410162329292.png)

但这两种方式都有弊端：但是当前的最佳结果不一定是全局的最佳结果。下棋就是很明显的例子，每一步都想吃对方，可能会中陷阱。

### Transition Model：概率

用于描述多步概率，比如吃豆人走“上上下下左右左右”这条路径的概率。
$$
P(s'|s,a)
$$
从s出发，走了a的行动后（比如[上，上，下……]）到达s'的概率。

概率这东西有什么用？我只选择分数最高的决策不就可以了吗？并非如此，一些决策问题中充斥着不确定性，比如吃豆人在当前状态下，往左和往右走活命概率是0.1，往上走活命概率是0.8. 这种不确定性无法准确归纳到分数中，只有和分数效用结合才能取得更好的答案。（下面会给出计算公式）

Transition Model 是一阶马尔科夫链，只和当前以及下一步的状态有关，所以我们可以根据当前状态开始往后推出所有概率的概率表。

除了行动概率，我们还需要一个算分数的激励函数来鼓励小吃豆人更有效地寻路（比如，用时更短？吃掉的豆子更多？等等）。分数可能包含在地图的点位上（路径点上哪些地方有食物，到达此点吃豆人加分），以及吃豆人每一步行动会消耗分数来鼓励吃豆人尽快到达目的地。

这个问题也叫做：

### Markov decision process (MDP)：效用

已知：

- s：当前位置。
- A(s)：在s处能采取哪些行动。
- P(s'|s,a)：transition model。
- R(s)：奖励函数，s点行动后得到的奖励，可能是正数也可能是负数。

马尔科夫决策过程最终期望得到：policy $\pi (s)$，比如 $\pi (s1)=left$ 表示在 s1 处往左走的决策。

optimum policy：最佳决策 $\pi ^*(s)$。

utilities：最终效用。效用有很多种，比如：

- 有限还是无限？比如吃豆人，吃完所有豆豆就结束游戏了，如果每一步都扣分数，吃掉豆豆加分，那么 utilities 明显是有限的，有最大值。
- stationary 可统计的吗？比如同一套决策，两次应用，会得到相同的结果吗？第二局鬼魂的行动方式会不会发生改变？今天看到阴天，明天会不会不下雨？
- additive：每一步效用相加=最终效用。discount：  $R(s1)+\gamma R(s2)++\gamma ^2 R(s3)+...$ γ是折扣因子，0-1。

例题：+1 -1是两个出口，最终效用是所有效用相加。在四个场景中给定四个R(s)值，看看计算机会偏向于怎么走。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202410162352374.png" alt="image-20241016235203319" style="zoom: 67%;" />

图1：每一步的损失太大了。吃豆人只想赶紧走到终点结束，哪怕是-1的终点，早些结束比绕路吃到+1的损失也小。

图2：损失小了一点，吃豆人尽可能避开-1的情况下朝着+1出口以最快速度前进。

图3：损失更小了，吃豆人显得没有那么在意每一步的损失了，不吃到-1的情况下，绕远路到+1都可以。损失不大。

图4：每一步都没有损失只有激励。吃豆人不想走到终点离开游戏，只想不停地在迷宫里徘徊，因为每一步都有加分。

图4的情况就可以用 discount 规避，如果想在迷宫里一直混分数，γR(s)得到的分数会越来越少。

如果在有限的决策步数内可以结束问题，那么每一步的平均奖励也可以作为评价结果的指标之一， expected utility。我们可以选择平均期望值更高的路径，这也是一种决策方法（感觉更有全局观一点）。
$$
\pi ^*=arg\; \mathop{max}\limits_{max}U^\pi (s)
$$
例题：图中给出了每个点位走到终点的期望效用。如果在左下角开始出发，走上路的期望效用要比走右路的大。

<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202410171636764.png" alt="image-20241017163646639" style="zoom:67%;" />

最终我们得到的最佳效用公式：结合了不确定性和效用。
$$
\pi ^*(s)=arg\; \mathop{max}\limits_{a \in A(s)}\sum_{s'}P(s'|s,a) U^{\pi ^*} (s')
$$
其中最后一项就是单步期望效用，也就是说我们每一次决策都走期望效用最大的决策方向。

### Bellman equation

那么问题就转化为：如何得到单步期望效用？每个状态的即时奖励期望 + 下一个状态的价值期望，不同的决策可能导致不同的下一个状态的价值期望不同，我们尝试所有的决策，最后取得期望效用最大的结果。
$$
U(s)=R(s)+\gamma \mathop {max}_{a \in A(s)}\sum_{s'}P(s'|s,a)U(s')
$$
例：从(1,1)出发，格子中写了每个点位的期望效用。

如果我想往一个方向走，会有0.2的概率往这个方向的左右两侧走。

下面的公式是计算了4个方向的决策的效用。最终我们选择最大值。

![image-20241017184758222](https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202410171847327.png)

当然，每次移动的-0.04成本也可以用函数 c(s, a) 表示，可以赋值来表达移动偏好。

### Policy Iteration 策略迭代

Policy evaluation：给定一个 policy （比如当前的策略是向上走），求效用 U(s)

Policy improvement：根据效用找到当前状态的最好策略，更新效用。如果没有更好的策略，说明效用已经收敛了，当前策略已经是最佳策略了。

![[强化学习基础篇: 策略迭代 (Policy Iteration) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/34006925)](https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202410171917677.webp)

如上例，首先所有点都采用“向下”的策略。然后计算期望效用，发现左右两列一直向下找不到宝藏需要更新策略，中间列还可以。

然后第一轮迭代，采用“横向移动”的策略，因为往中间列走可以带来更大的效用。而中间列不动。左右两列期望值变大了，那么左右两列最优策略更新.

但是这样计算量可能会很大（n个线性方程，每个有3种策略的话，计算量可能达到n^3^）所以我们取得近似值即可。